# Data Science & Machine Learning Internship

**Company:** Brainybeam Info-Tech PVT LTD  
**Duration:** 26 May 2025 â€“ 24 June 2025  
**Domain:** Data Science & Machine Learning  

---

## ðŸ“Œ Introduction
This repository contains all the work, tasks, and projects completed during my internship at **Brainybeam Info-Tech PVT LTD**.  
The internship focused on core **Data Science & Machine Learning concepts**, practical implementations in **Python (Jupyter Notebooks)**, and two major projects.

---

## ðŸ“… Daily Task Log

### ðŸ—“ Week 1 (26th May â€“ 1st June 2025)
**Day 1 (26th May 2025): Introduction & Setup**  
- Orientation, environment setup, Jupyter Notebook installation, and Python refresher.  

**Day 2 (27th May 2025): Python Basics for Data Science**  
- Worked on Python fundamentals including lists, tuples, dictionaries, loops, and functions.  

**Day 3 (28th May 2025): NumPy & Pandas**  
- Explored NumPy arrays, array operations, Pandas DataFrames, and Series.  

**Day 4 (29th May 2025): Data Preprocessing**  
- Handled missing values, duplicate removal, categorical encoding, and feature scaling.  

**Day 5 (30th May 2025): Data Visualization**  
- Used Matplotlib & Seaborn for histograms, scatter plots, heatmaps, and correlation matrices.  

**Day 6 (31st May 2025): Exploratory Data Analysis (EDA)**  
- Performed EDA on sample datasets, derived insights using summary statistics.  

**Day 7 (1st June 2025): Mini Project â€“ Data Cleaning**  
- Applied preprocessing techniques on a real dataset and documented workflow.  

---

### ðŸ—“ Week 2 (2nd June â€“ 8th June 2025)
**Day 8 (2nd June 2025): Introduction to Machine Learning**  
- Learned supervised vs unsupervised learning, ML pipeline concepts.  

**Day 9 (3rd June 2025): Regression Models**  
- Implemented Linear Regression and Polynomial Regression.  

**Day 10 (4th June 2025): Classification Models**  
- Implemented Logistic Regression & K-Nearest Neighbors (KNN).  

**Day 11 (5th June 2025): Decision Trees & Random Forest**  
- Built models, checked feature importance, and compared accuracy.  

**Day 12 (6th June 2025): Model Evaluation**  
- Learned about confusion matrix, precision, recall, F1-score, ROC-AUC.  

**Day 13 (7th June 2025): Support Vector Machines (SVM)**  
- Implemented SVM with different kernels, performed hyperparameter tuning.  

**Day 14 (8th June 2025): Mini Project â€“ Classification**  
- Applied ML algorithms to classify dataset, compared performances.  

---

### ðŸ—“ Week 3 (9th June â€“ 15th June 2025)
**Day 15 (9th June 2025): Clustering Techniques**  
- Implemented K-Means, Hierarchical, and DBSCAN clustering.  

**Day 16 (10th June 2025): Dimensionality Reduction**  
- Applied PCA & t-SNE for feature reduction.  

**Day 17 (11th June 2025): Feature Engineering**  
- Created new features, handled skewed data, normalized values.  

**Day 18 (12th June 2025): Time Series Analysis**  
- Explored ARIMA models and moving averages.  

**Day 19 (13th June 2025): Natural Language Processing (NLP)**  
- Tokenization, stopword removal, TF-IDF, and sentiment analysis.  

**Day 20 (14th June 2025): Model Deployment Basics**  
- Learned about Flask API for ML model deployment.  

**Day 21 (15th June 2025): Mini Project â€“ NLP Sentiment Analysis**  
- Built sentiment analysis model using NLP techniques.  

---

### ðŸ—“ Week 4 (16th June â€“ 22nd June 2025)
**Day 22 (16th June 2025): Project 1 â€“ Data Preprocessing Pipeline**  
- Started working on automated preprocessing workflow.  

**Day 23 (17th June 2025): Project 1 â€“ Feature Engineering & Scaling**  
- Implemented transformations, scaling methods, and outlier detection.  

**Day 24 (18th June 2025): Project 1 â€“ Model Training**  
- Integrated classification models with the preprocessing pipeline.  

**Day 25 (19th June 2025): Project 1 â€“ Evaluation & Documentation**  
- Evaluated performance metrics, documented workflow.  

**Day 26 (20th June 2025): Project 2 â€“ Predictive Modeling**  
- Started predictive modeling project with real dataset.  

**Day 27 (21st June 2025): Project 2 â€“ Data Analysis & Model Selection**  
- Analyzed dataset, selected candidate ML algorithms.  

**Day 28 (22nd June 2025): Project 2 â€“ Model Training & Tuning**  
- Hyperparameter tuning, cross-validation for selected models.  

---

### ðŸ—“ Final Week (23rd â€“ 24th June 2025)
**Day 29 (23rd June 2025): Project 2 â€“ Final Evaluation & Results**  
- Generated evaluation metrics, visualizations, and insights.  

**Day 30 (24th June 2025): Internship Wrap-Up**  
- Completed documentation, prepared final report, and presented outcomes.  

---

## ðŸš€ Projects

### **Project 1: Building SVM and k-NN Classifiers from Scratch**
**Abstract:**  
Build advanced models from scratch, such as a Support Vector Machine (SVM) and k-Nearest Neighbors (k-NN).  
For SVM, implement the kernel trick (linear, polynomial, and RBF) manually to map input data into higher-dimensional spaces and maximize the margin between classes.  
For k-NN, manually implement distance metrics (Euclidean, Manhattan, etc.) and the k-selection process using the Elbow Method, determining the optimal number of neighbors for classification.  
Avoid using high-level libraries like Scikit-learn for these tasks, and ensure both models are optimized and fully functional.

**Keywords:** SVM from scratch, kernel trick, polynomial kernel, RBF kernel, k-NN implementation, Euclidean distance, Manhattan distance, Elbow Method, manual machine learning, optimization, Python  

**Technology:** Data Science & Machine Learning  

---

### **Project 2: Time-Based Stock Price Prediction for TSLA**
**Abstract:**  
Partition the dataset into training and testing subsets using an 80/20 time-based split.  
Develop a Linear Regression model for TSLA price prediction, incorporating lag features and technical indicators.  
Evaluate the model's performance using MAE, MSE, RMSE, and RÂ².  
Follow up with Decision Tree and Random Forest models, optimizing hyperparameters and visualizing feature importance.

**Keywords:** TSLA stock prediction, time series split, lag features, technical indicators, linear regression, decision tree, random forest, model evaluation, feature importance, hyperparameter tuning, financial forecasting, stock price modeling  

**Technology:** Data Science & Machine Learning  

---

## ðŸ“Š Project Work Timeline 

- **Week 3 (9 June â€“ 15 June):**  
  Started **Project 1 (SVM & k-NN Classifiers from Scratch)**
  â€“ Understood project requirements and outlined the workflow.
  â€“ Implemented distance metrics (Euclidean, Manhattan) for k-NN.
  â€“ Applied the Elbow Method for selecting the optimal k-value.
  â€“ Designed custom kernel functions (linear, polynomial, RBF) for SVM.
  â€“ Tested classifiers on synthetic and real-world datasets.
  â€“ Compared performance with sklearnâ€™s inbuilt SVM & k-NN.
  â€“ Documented findings, accuracy scores, and limitations.

- **Week 4 (16 June â€“ 24 June):**  
  Worked on **Project 2 (Stock Price Prediction for TSLA)**
  â€“ Defined objectives and planned dataset preparation strategy.
  â€“ Collected and preprocessed historical Tesla stock price data.
  â€“ Performed feature engineering (moving averages, technical indicators).
  â€“ Implemented multiple regression models: Linear Regression, Random Forest, XGBoost.
  â€“ Conducted hyperparameter tuning using GridSearchCV.
  â€“ Evaluated models with MSE, RMSE, and RÂ² score.
  â€“ Visualized actual vs predicted stock prices using Matplotlib & Seaborn.
  â€“ Compared modelsâ€™ performance and identified the best-fit approach.
  â€“ Compiled project report summarizing methodology, results, and insights.  

---

## âœ… Conclusion
During this internship, I gained hands-on experience in:  
- Python for Data Science  
- Data preprocessing, visualization, and analysis  
- Implementing ML models from scratch  
- Time-series forecasting and regression techniques  
- Applying optimization and evaluation metrics  

This internship helped strengthen my **machine learning foundations** and improve my ability to work on **real-world projects**.

---
